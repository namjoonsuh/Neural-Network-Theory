# Theory on Neural Network Models

We use this repository to keep track of slides that we are making for a theoretical review on neural network based models. 

## Useful links

* A Princeton course on deep learning theory: https://github.com/leiwu1990/course.math_theory_nn
* Slides for a summer school on Deep Learning at SJTU 2020: folder "*dl summer school 2020*"

## Table of contents

The following is a list of papers that we are working on presentatoin slides. 

  * The PDF files of the corresponding papers are in folder "papers". 
  * The corresponding Latex sources are in folder "slides source files". 

1. Nonparametric regression using deep neural networks with ReLU activation function; J Schmidt-Hieber - arXiv preprint arXiv:1708.06633, 2017 

  * *papers*/1708.06633.pdf 
  * *slides source files*/Hieber_approx.xxx for the functional approximation part
  * *slides source files*/Hieber_Risk.xxx for the minimax estimation rate part 

2. Optimal approximation of piecewise smooth functions using deep ReLU neural networks; P Petersen, F Voigtlaender - Neural Networks, 2018 - Elsevier

  * *papers*/1709.05289.pdf
  * *slides source files*/Petersen.xxx

3. Error bounds for approximations with deep ReLU networks; D Yarotsky - Neural Networks, 2017 - Elsevier

  * *papers*/1610.01145.pdf
  * *slides source files*/Yarotsky.xxx

_The following papers are possibly in the pipeline._

4. Universality of deep convolutional neural networks; DX Zhou - Applied and computational harmonic analysis, 2020 - Elsevier

  * *papers*/1805.10769.pdf

5. Fast learning rates for plug-in classifiers; JY Audibert, AB Tsybakov - The Annals of statistics, 2007

  * *papers*/1183667286.pdf 

6. Optimal aggregation of classifiers in statistical learning; AB Tsybakov - The Annals of Statistics, 2004

  * *papers*/1079120131.pdf 

7. Smooth discrimination analysis; E Mammen, AB Tsybakov - The Annals of Statistics

  * *papers*/1017939240.pdf 
  
8. A Theoretical Analysis of Deep Q-Learning; Jan et al. (2020); A theoretical analysis of the deep reinforcement learning.

 * *papers*/1901.00137.pdf [https://arxiv.org/pdf/1901.00137.pdf]

9. Understanding Implicit Regularization in Over-Parameterized Nonlinear Statistical Model.

 * *papers*/2007.08322.pdf [https://arxiv.org/abs/2007.08322]
 
10. Gradient Descent Provably Optimizes Over-parameterized Neural Networks

 * *papers*/1810.02054.pdf [https://arxiv.org/abs/1810.02054]
 
11. ROOT-SGD: Sharp Nonasymptotics and Asymptotic Efficiency in a Single Algorithm

 * *papers*/ [http://www.optimization-online.org/DB_FILE/2020/08/7979.pdf]
